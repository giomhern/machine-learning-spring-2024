{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ce827a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "\n",
    "### DATA 22100 - Introduction to Machine Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/UChicago_DSI.png\" align=\"right\" alt=\"UC-DSI\" width=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb79567",
   "metadata": {},
   "source": [
    "<center> \n",
    "\n",
    "# (Simple) Recurrent Neural Networks\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "</center> \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ac5c6",
   "metadata": {},
   "source": [
    "### RNNs vs. Feed-forward networks\n",
    "\n",
    "|   |   |  |\n",
    "|:-|:-:|:-|\n",
    "| **Feed-forward netwok** | &nbsp; &nbsp; &nbsp; | **Recurrent Network** | \n",
    "| Fixed input size (e.g., image)  | | Variable input size |\n",
    "| All input taken in at once      | | Input sequence taken one vector at a time |\n",
    "| Fixed output size (e.g., class) | | Variable output size |  \n",
    "\n",
    "<br/> \n",
    "\n",
    "### At each step the RNN considers previous steps through the hidden state \n",
    "\n",
    "* A **hidden state** vector is initialized to zeros. \n",
    "* At each step, the RNN considers an input vector (one of a sequence of input vectors) along with the previous hidden state.\n",
    "* The previous hidden state and the input vector are both used by the RNN cell to calculate the next hidden state (and output). \n",
    "* The final result **depends on all the previous input vectors** through the hidden states that were passed along.\n",
    "* A typical caculation, e.g., performed by [`torch.nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) is:\n",
    "\n",
    "$$\\begin{eqnarray} \n",
    "\\text{next\\_hidden} &=& \\tanh\\left[\\text{input\\_item} \\times \\text{input\\_weight(s)} + \\text{input\\_bias(es)} \\right. \\\\\n",
    "&& \\hspace{10mm} \\left. + \\text{previous\\_hidden} \\times \\text{hidden\\_weight(s)} + \\text{hidden\\_bias(es)}  \\right]\n",
    "\\end{eqnarray}$$ \n",
    "\n",
    "Or, adopting the terminology of **time-steps**:\n",
    "\n",
    "$$\\begin{eqnarray} \\text{hidden}_t &=&  \n",
    "\\tanh\\left[\\text{input}_{t} \\  \\text{weights}_{\\text{input}} + \\text{biases}_{\\text{input}} + \\text{hidden}_{t-1} \\ \\text{weights}_{\\text{hidden}} + \\text{biases}_{\\text{hidden}}  \\right] \n",
    "\\end{eqnarray}$$\n",
    "\n",
    "* The argument of the $\\tanh(.)$ function is a linear layer. \n",
    "* There are variations on this theme. For instance, $ReLu(.)$ can replace the $\\tanh(.)$ \n",
    "\n",
    "<br> \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/RNN_animation.gif\" width=\"300\">\n",
    "\n",
    "\n",
    "<br> \n",
    "\n",
    "#### 'Unrolled' diagrams of RNNs are common\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/RNN_diagram.png\" width=\"450\">\n",
    "\n",
    "This can be somewhat confusing. **Note:** \n",
    "\n",
    "The **same** RNN cell is reused throughout: <br> only one set of weights (and biases) is updated.\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e5b6d",
   "metadata": {},
   "source": [
    "\n",
    "| |  |\n",
    "|:-:|:-| \n",
    "|<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/icon_example.png\" width=\"50\">| A contrived example with arbitrary numbers. Each niput item is a vector with two elements. <br> A sigle output is calculated after the a sequence of two inputs. Red: input. Yellow: current hidden state. <br> Green: next hidden state. For simplicity: hidden state weights are $1$ and biases are $0$ (not shown). | \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/RNNcontrivedExample.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98e70f",
   "metadata": {},
   "source": [
    "#### Different tasks may require sequences of inputs, output, or both\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/RNN_types.png\" width=\"600\">\n",
    "\n",
    "* Each rectangle is a vector \n",
    "* Input vectors are red\n",
    "* Output vectors are blue\n",
    "* Hidden state vectors are green\n",
    "\n",
    "#### Recomended reading \n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks (2015)](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945dab5e",
   "metadata": {},
   "source": [
    "### The goal:\n",
    "\"You give it a large chunk of text and it will learn to generate text like it **one character at a time**.\"\n",
    "\n",
    "We'll frame this like a **classification task**: \n",
    "* The desired product of the model (after considering an entire sequence of characters) is a single output - the most likely next character. \n",
    "* Each possible character (e.g., lowercase letter or space) will be a class. \n",
    "* The model will produce a vector of class probability scores from which we can choose the most likely class/character. \n",
    "\n",
    "\n",
    "<br/> \n",
    "\n",
    "#### Supervised learning\n",
    "\n",
    "During training:\n",
    "* Each sequence of training characters (represented by a sequence of vectors) has a corresponding target character or sequence of characters that the model should learn. \n",
    "* Forward pass $\\rightarrow$ prediction and loss\n",
    "* Backward pass $\\rightarrow$ gradient \n",
    "* Optimizer step $\\rightarrow$ update weights \n",
    "* Zero the gradient, etc. \n",
    "\n",
    "<br/> \n",
    "\n",
    "#### One-hot-encoding: a naive vector represention of characters \n",
    "\n",
    "|   |   | \n",
    "|:-:|:-:|\n",
    "| <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/icon_example.png\" width=\"50\"> | One-hot-encoding for a list of four characters: \\[ 'w', 'x', 'y', 'z' \\] | \n",
    "| 'w' | \\[ 1, 0 , 0, 0 \\] |\n",
    "| 'x' | \\[ 0, 1, 0 , 0 \\] |\n",
    "| 'y' | \\[ 0, 0, 1 , 0 \\] |\n",
    "| 'z' | \\[ 0, 0, 0 , 1 \\] |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "756b1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "char_list = [[c] for c in 'wxyz'] # will get converted to 2d-array\n",
    "enc = OneHotEncoder() \n",
    "print(enc.fit_transform(char_list).toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d443a8b",
   "metadata": {},
   "source": [
    "|   |   | \n",
    "|:-:|:-|\n",
    "| <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/icon_comment.png\" width=\"50\"> | A more sophisticated approach could treat each **word** as a single  <br> token and use word embedding, e.g., [BERT word embedding](https://en.wikipedia.org/wiki/BERT_(language_model)), <br> to encode them as vectors. We will not implement that here <br> (the examples below use smaller data and a simpler model). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293a9b2",
   "metadata": {},
   "source": [
    "### Implementing a simple RNN manually (sort of)\n",
    "\n",
    "`PyTorch` implements a [multilayer RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) but for startyers we'll implement our own. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9d5db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchvision # ==0.14.1\n",
    "# !pip3 install torch # ==1.13.1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac935741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f02e6d4",
   "metadata": {},
   "source": [
    "### Individual input items \n",
    "\n",
    "Individual lowercase alphabetic characters, a space character, and \"'\". \n",
    "\n",
    "All 28 characters are one-hot encoded (each represented by a vector).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50c57bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.] --> a\n"
     ]
    }
   ],
   "source": [
    "chars = \" 'abcdefghijklmnopqrstuvwxyz\"\n",
    "char_list = [[c] for c in chars] # will get converted to 2d-array\n",
    "enc = OneHotEncoder() \n",
    "enc.fit(char_list)\n",
    "\n",
    "# Being able to go back from a vector \n",
    "# to a characterwill be useful: \n",
    "def vec_to_char(vec, enc): \n",
    "    '''\n",
    "    Input: one-hot encoded vector and the encoder. \n",
    "    Output: corresponding character from \n",
    "            \" 'abcdefghijklmnopqrstuvwxyz\"\n",
    "    '''\n",
    "    if isinstance(vec, np.ndarray):\n",
    "        idx = np.argmax(vec)\n",
    "    elif isinstance(vec, torch.Tensor): \n",
    "        idx = torch.argmax(vec).item()\n",
    "    else: \n",
    "        return None \n",
    "    return enc.categories_[0][idx]\n",
    "\n",
    "# Test the encoding and decoding: \n",
    "one_hot = enc.transform(char_list).toarray()\n",
    "print(one_hot[2],'-->',vec_to_char(one_hot[2], enc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82645ba9",
   "metadata": {},
   "source": [
    "### The training data: a para from a (great) book\n",
    "#### (somewhat cleaned)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "606072ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"It was like the second when you come home late at night and \"\n",
    "text += \"see the yellow envelope of the telegram sticking out from \"\n",
    "text += \"under your door and you lean and pick it up, \"\n",
    "text += \"but don't open it yet, not for a second. While you stand \"\n",
    "text += \"there in the hall, with the envelope in your hand, \"\n",
    "text += \"you feel there's an eye on you, a great big eye looking \"\n",
    "text += \"straight at you from miles and dark and through walls \"\n",
    "text += \"and houses and through your coat and vest and hide and \"\n",
    "text += \"sees you huddled up way inside, in the dark which is you, \"\n",
    "text += \"inside yourself, like a clammy, sad little foetus you \"\n",
    "text += \"carry around inside yourself. The eye knows \"\n",
    "text += \"what's in the envelope, and it is watching you to see you \"\n",
    "text += \"when you open it and know, too. But the clammy, sad little \"\n",
    "text += \"foetus which is you way down in the dark which is you too \"\n",
    "text += \"lifts up its sad little face and its eyes are blind, \"\n",
    "text += \"and it shivers cold inside you for it doesn't want to \"\n",
    "text += \"know what is in that envelope. It wants to lie in the \"\n",
    "text += \"dark and not know, and be warm in its not-knowing. \"\n",
    "\n",
    "text += \"The end of man is knowledge, but there is one thing \"\n",
    "text += \"he can't know. He can't know whether knowledge will \"\n",
    "text += \"save him or kill him. He will be killed, all right, \"\n",
    "text += \"but he can't know whether he is killed because of the \"\n",
    "text += \"knowledge which he has got or because of the knowledge \"\n",
    "text += \"which he hasn't got and which if he had it, would save \"\n",
    "text += \"him. There's the cold in your stomach, but you open the \"\n",
    "text += \"envelope, you have to open the envelope, for the end of \"\n",
    "text += \"man is to know.\"\n",
    "\n",
    "text = text.lower()\n",
    "text = text.replace('.','')\n",
    "text = text.replace(',','')\n",
    "text = text.replace('-','')\n",
    "# set(''.join(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1451cf",
   "metadata": {},
   "source": [
    "### Simple RNN using `torch.nn.RNN`\n",
    "\n",
    "**For simplicity:**  \n",
    "\n",
    "* Divide the training text to fixed length sequences of characters. \n",
    "* Pad the last sequence with spaces as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc074ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \" 'abcdefghijklmnopqrstuvwxyz\"\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebd4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it was like the sec', 'nd when you come ho', 'e late at night and']\n",
      "['t was like the seco', 'd when you come hom', ' late at night and ']\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "tmp = [(text[c:c+N]) \n",
    "        for c in range(0, len(text), N)]\n",
    "tmp[-1] = tmp[-1]+' '*(N-len(tmp[-1]))\n",
    "\n",
    "X_train = [x[:-1] for x in tmp]\n",
    "Y_train = [y[1:] for y in tmp]\n",
    "\n",
    "print(X_train[:3])\n",
    "print(Y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b28d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 21, 0, 24, 2, 20, 0, 13, 10, 12, 6, 0, 21, 9, 6, 0, 20, 6, 4]\n",
      "[21, 0, 24, 2, 20, 0, 13, 10, 12, 6, 0, 21, 9, 6, 0, 20, 6, 4, 16]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] = [char2int[c] for c in X_train[i]]\n",
    "    Y_train[i] = [char2int[c] for c in Y_train[i]]\n",
    "\n",
    "print(X_train[0])\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d5f6d",
   "metadata": {},
   "source": [
    "#### One-hot encode each input sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "450a8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor(6.)\n",
      "tensor(6.)\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([70, 19, 28]) torch.Size([70, 19])\n"
     ]
    }
   ],
   "source": [
    "N_chars = len(chars)\n",
    "seq_len = N - 1\n",
    "batch_size = len(X_train)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    '''\n",
    "    Input: a list of sequences of integers (of a fixed length).\n",
    "    Output: a torch.tensor with the one-hot encoded sequences. \n",
    "    '''\n",
    "    vec = np.zeros((batch_size, seq_len, dict_size), \n",
    "                    dtype=np.float32)\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            vec[i, u, sequence[i][u]] = 1\n",
    "    return torch.tensor(vec)\n",
    "\n",
    "X_train_enc = one_hot_encode(X_train, N_chars, seq_len, batch_size)\n",
    "\n",
    "Y_train_enc = torch.Tensor(Y_train)\n",
    "\n",
    "print(X_train_enc[0][0])  # 'i'\n",
    "print(X_train_enc[0][8])  # 'i' \n",
    "print(Y_train_enc[1][4])  # 'e' \n",
    "print(Y_train_enc[1][14]) # 'e'\n",
    "print(type(X_train_enc), type(Y_train_enc))\n",
    "print(X_train_enc.shape, Y_train_enc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0660782",
   "metadata": {},
   "source": [
    "#### A model with a single `torch.nn.RNN` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43c19511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(myRNN, self).__init__()\n",
    "\n",
    "        # Create hidden_size attribute\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Create one RNN Layer\n",
    "        self.rnn = nn.RNN(input_size=input_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=1, \n",
    "                          batch_first=True)   \n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input (see below)\n",
    "        # hidden = self.init_hidden(batch_size)\n",
    "        hidden = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        \n",
    "        # input + hidden state --> outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs for the linear layer\n",
    "        # (contiguous in memory for performance:\n",
    "        #  improving data locality improves performance because \n",
    "        #  memory access is more efficient)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98202671",
   "metadata": {},
   "source": [
    "#### Instantiate the model, ser hyperparameters, choose loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "154acdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with dimensions:\n",
    "# Each input: single one-hot-encoded character (length len(chars))\n",
    "# Each output: single one-hot-encoded character (length len(chars))\n",
    "# Each hidden state: a tensor of length 12 (arbitrary)\n",
    "# n_layers=1: a single RNN layer (in principle they could be stacked)\n",
    "\n",
    "model = myRNN(input_size=len(chars), \n",
    "              output_size=len(chars), \n",
    "              hidden_size=12)\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 2000\n",
    "lr=0.01\n",
    "\n",
    "# Loss function: CrossEntropyLoss() is populaar for classification\n",
    "loss_func = nn.CrossEntropyLoss() \n",
    "\n",
    "# Optimizer: Adam, AdamW, or SGD are popular choices \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc0ca4a",
   "metadata": {},
   "source": [
    "#### Train the RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c37b2438-d21d-48aa-a14d-5a7015dbf63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1330, 28])\n",
      "Y_train_enc shape: torch.Size([70, 19])\n",
      "Y_train_enc.view(-1) shape: torch.Size([1330])\n"
     ]
    }
   ],
   "source": [
    "out, hid = model(X_train_enc)\n",
    "print('Output shape:',out.shape)\n",
    "print('Y_train_enc shape:', Y_train_enc.shape)\n",
    "print('Y_train_enc.view(-1) shape:', Y_train_enc.view(-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1319210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2000... Loss: 3.4294\n",
      "Epoch: 100/2000... Loss: 1.8656\n",
      "Epoch: 200/2000... Loss: 1.4681\n",
      "Epoch: 300/2000... Loss: 1.2910\n",
      "Epoch: 400/2000... Loss: 1.2269\n",
      "Epoch: 500/2000... Loss: 1.1820\n",
      "Epoch: 600/2000... Loss: 1.1653\n",
      "Epoch: 700/2000... Loss: 1.1617\n",
      "Epoch: 800/2000... Loss: 1.1259\n",
      "Epoch: 900/2000... Loss: 1.1139\n",
      "Epoch: 1000/2000... Loss: 1.1114\n",
      "Epoch: 1100/2000... Loss: 1.0937\n",
      "Epoch: 1200/2000... Loss: 1.1493\n",
      "Epoch: 1300/2000... Loss: 1.0824\n",
      "Epoch: 1400/2000... Loss: 1.0773\n",
      "Epoch: 1500/2000... Loss: 1.0530\n",
      "Epoch: 1600/2000... Loss: 1.0563\n",
      "Epoch: 1700/2000... Loss: 1.0479\n",
      "Epoch: 1800/2000... Loss: 1.0347\n",
      "Epoch: 1900/2000... Loss: 1.0448\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    output, hidden = model(X_train_enc)\n",
    "    loss = loss_func(output, Y_train_enc.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('Epoch: {}/{}... Loss: {:.4f}'.format(epoch, \n",
    "                                                    n_epochs,\n",
    "                                                    loss.item()))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f24cd312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you ope you ope in the'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From model and character predict the next character and hidden state\n",
    "def predict(model, char_seq):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    char_seq = np.array([[char2int[c] for c in char_seq]])\n",
    "    char_seq = one_hot_encode(char_seq, N_chars, \n",
    "                              char_seq.shape[1], 1)\n",
    "    \n",
    "    out, hidden = model(char_seq)\n",
    "\n",
    "    # Choose class with the highest probability from the output\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "\n",
    "    return int2char[char_ind], hidden\n",
    "\n",
    "# Return sentence from desired output length and input characters\n",
    "def sample(model, out_len, start):\n",
    "\n",
    "    # model.eval() is a 'switch' for layers/parts of the model that \n",
    "    #              behave differently during training and predicting\n",
    "    #              (e.g., Dropouts Layers). It is common practice to\n",
    "    #              predict with torch.no_grad() and model.eval() \n",
    "    model.eval() # eval mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = start.lower()\n",
    "        # Run through the starting characters\n",
    "        chars = [ch for ch in start]\n",
    "        size = out_len - len(chars)\n",
    "        \n",
    "        # Pass in the previous characters and get a new one\n",
    "        for _ in range(size):\n",
    "            new_char, _ = predict(model, chars)\n",
    "            chars.append(new_char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "\n",
    "sample(model, 25, \"do you\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9cd55",
   "metadata": {},
   "source": [
    "Not quite English but the model is extremely simple and the training dataset is ridiculously small... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c67d093",
   "metadata": {},
   "source": [
    "### The vanishing/exploding gradient problem \n",
    "#### (or why simple RNNs are not very useful in and of themselves) \n",
    "\n",
    "Generally, a 'vanishing/exploding gradient problem' occurs when the gradients become too small or too large during backpropagation. \n",
    "\n",
    "* Gradient too small $\\rightarrow$ gradient descent cannot effectively proceed (not to mention underflow). \n",
    "* Gradient too large $\\rightarrow$ gradient descent cannot effectively converge (not to mention overflow). \n",
    "\n",
    "#### RNNs are prone to this problem because of the recurrent connections. \n",
    "\n",
    "* Consider a somewhat long (but not unheard of) sequence of $100$ input items. \n",
    "* Consider a wieght, $w$, that gets reused $100$ times as the RNN is 'unrolled'.  \n",
    "* $\\frac{d \\ loss}{d \\ weight} \\to 0 \\ \\ \\ \\text{ or } \\infty$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0189e40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you ope you ope i'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 20, \"do you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8bb246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you ope you ope in the yo  ouro enkye lope in the hall the yes wide in the envelope in your and i'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, 100, \"do you\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
