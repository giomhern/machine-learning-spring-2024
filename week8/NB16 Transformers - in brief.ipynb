{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ce827a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "\n",
    "### DATA 22100 - Introduction to Machine Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/UChicago_DSI.png\" align=\"right\" alt=\"UC-DSI\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb79567",
   "metadata": {},
   "source": [
    "<center> \n",
    "\n",
    "# A brief intro to Transformers\n",
    "    \n",
    "<br>\n",
    "    \n",
    "</center> \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b497e76",
   "metadata": {},
   "source": [
    "## A brief and incomplete history \n",
    "\n",
    "(Adapted from a presentation by Brit Cruise)\n",
    "\n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deffa95",
   "metadata": {},
   "source": [
    "### Supervised learning networks\n",
    "\n",
    "* Training realies on comparing predictions with known outcomes (loss).\n",
    "* The trained networks can perform **a specific task** well but cannot perform a different one. \n",
    "* The trained network requires a specific shape of input. \n",
    "* Networks could not be 'naturally' tweaked to create general purpose systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35ec6c",
   "metadata": {},
   "source": [
    "### [Learning representations by back-propagating errors (1986) Rumelhart, Hinton & Williams ](https://www.nature.com/articles/323533a0)\n",
    "\n",
    "* Using the chain rule for backpropagation (of errors) by computares dates back to the 1960s. \n",
    "* Neural networks were first trained using backpropagation in 1986. \n",
    "\n",
    "\n",
    "### [A Parallel Distributed Processing Approach (1986) Jordan (the other one)](https://cseweb.ucsd.edu/~gary/PAPER-SUGGESTIONS/Jordan-TR-8604-OCRed.pdf)\n",
    "\n",
    "* A small network, trained using backpropagation, can learn sequential patterns. \n",
    "    * Each training cycle prediced one letter at the end of the sequence (supervised learning). \n",
    "\n",
    "| | | \n",
    "|:-:|:-:|\n",
    "| <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersHistoryJordan1986network.png\" width=\"250\"> | <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersHistoryJordan1986table.png\" width=\"230\"> |\n",
    "\n",
    "* The neurons connected (downstream) to the output, (upstream) to the hidden layer, and to them selves were called 'state units'. \n",
    "    * They were supposed to maintain a 'state of mind' through which past examples can affect future predictions.\n",
    "    * This is an RNN... \n",
    "    \n",
    "* When training was over, the network **generated the pattern it learned**: starting with the first character, each output was the input for the next character. \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0441e95d",
   "metadata": {},
   "source": [
    "### [Finding Structure in Time (1990) Elman](https://pdf.sciencedirectassets.com/272076/1-s2.0-S0364021300X00602/1-s2.0-036402139090002E/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEGAaCXVzLWVhc3QtMSJHMEUCIQCrkhUzIj3ozWQFweq92cccOG8JpaabCD%2BhSYlv0W%2FAdQIgO1QHbMn%2FkGzrShNwcL4vvo93bS%2B%2BYjovCrwzHCXey08quwUIqf%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDORU8zqyY3D0o%2FymZiqPBef7yz%2BSTtbYUscNqfg0W4him9suJ6JDQIIj3GjnDixhIBjOOayk7rIAs72Tj6g5SR1HICw%2FFSREMtNSzCd6G1RcUjUpHD2%2B8iko1pvMkC0QCL1TpAByHHoV%2F0LGLgIjm7%2FfEQ5Zqk7dNZBLNeQ%2BF64e2Pz5DZazVBRYf3kkIKjSpx0pk161JM3xKiM6dbsxfWkfic8xFna9OFPCxvbI9uMGPD%2FtIEPaP2vZUQ%2F88%2BqmZmdcp4zLo2tjjwizNHp9aJnHh3sBqNkayRTdLcV2VnyLgYpZFkY87f8%2BkoIvR%2B0IQ2kzhM%2FeeupnZLTEs8J1W03m79fRZ34%2Fwr7XyLFPRyRzcnGd7zZAYGSWO9c8g%2FaOgDve7wrbZ%2F0rQxYYhjOb7A8ZSEuYita69UOzI0SrMDMALEyJkBf22x1z74JUY4WrH81cg5OhA9JM6Sry4bkcvTpocWhmlSDDKrxOGn7jPxnEx4YDc6zOmlHjVRZ%2Fbb1jpIoOeqh66Sfc8hNfyXwJIHpqoF6Ll6fw4CgHqMjwP2A5HDsenVxzZVZUAe52HCGoserYYbjtmNKSqDIFlvJJxU%2FzL9Y7whCIYXPzj6D3%2B9jcgJZ%2BYJJRhxp0lYO8UVDI%2BRcKhki1NEVo9qYOPItC8jggihQhKcdwYkN3viEiJdVeC1r%2FxQiYcY6IQ3E4o9FrFQ%2FyWMOU%2BI3GBxKnzyKkovvtnpmLMKxbq70SBS1h2g6sX8VudI9oTk2e3jIzJae%2BCb9Oq83QI7dS1J%2BfAiQal6uOP7XRavKoZOCh3h9FV4vAp%2FCmr%2FoSsfmxihyuECHvxB8CyrYJVsxn%2FlJr0t%2FpSOclih2XUEG2TSmYtlW4lzHLTm9Xg5%2FfYNjt7G85ArUww%2FepsQY6sQE4DPY2fgR%2FUOyYLJ4J%2BQf67Ax0LPmbJCCc%2FXozMUacCRQDQH7sa8BZaIKG6PAlWPsM%2BykrbNE9ruRHLqov0oc4urmXTnwhpsDmlh%2BWejAWtxy4G%2FgPD3p7sM2S67l2ZsXEi%2FemIGbjSuIJ0zKcwUw%2FdDtTryy%2Fbtbro04WahI2i85DVJjj%2FaY7HwaMI0sMfyJeB9vdetcvE42utlqcdwy0bIfCyo2fKN1AvZfO8BCFlms%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240425T170254Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYQENFGP6X%2F20240425%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d432bb3105dcfdbc9c051fb5fd48e318523424a069fa6be6b80677da63a25ba0&hash=b86c0bc346e292da0fc79c069396abe2b9fc13f7199ddeea28df40b9747c7234&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=036402139090002E&tid=spdf-1996de9a-ae15-4a92-88d0-76d34002d3d5&sid=3d74a3ec630c134f403b3c3751dd66dd3093gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f1258505c010156545a&rr=879fe7442905113e&cc=us) \n",
    "\n",
    "* A similar architecture with RNN ('context units') was expaned: \n",
    "    * 6 input units, 20 hidden units, 6 output units, and 20 context units.\n",
    "    \n",
    "* The network was trained on 200 short sentences (from four to nine words), all using a lexicon of 15 English language words. \n",
    "    * The training set contained $\\sim 5,000$ characters encoded in $5$-bit vectors. \n",
    "    * The training task was to predict the next character. \n",
    "    \n",
    "* The task of the train network was to predict a sequence of character starting from a single character prompt (each output would be used as the next input). \n",
    "\n",
    "#### Discovering words\n",
    "\n",
    "* Word boundaries (spaces) were omitted from the training set... \n",
    "    * ...but were leared! \n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersHistoryElman1990DiscoveringWords.png\" width=\"400\">\n",
    "    \n",
    "    (At the boundaries of words the RMS error peaked since within a word the next character is more predictable)\n",
    "    \n",
    "#### Discovering lexical classes ('meaning') \n",
    "\n",
    "* Larger network: the input layer and output layers contained 31 neurons each, and the hidden and context layers contained 150 nodes each.\n",
    "\n",
    "* Training task: learn to predict the order of successive words.\n",
    "    * Each word in the sequence was input, one at a time, in order.\n",
    "    * The task on each input cycle was to predict the next word in the sequence.\n",
    "    * Loss: RMS error between output word (one-hot-encoded) and a vector of the probabilities of each possible word being the next one (calculated from the training corpus). \n",
    "    \n",
    "* After training:\n",
    "    * The input was forward passed through the network one final time.\n",
    "    * All hidden unit activation patterns produced by a given word (in all its contexts) were averaged. \n",
    "    * These internal representations (word embedding) were then subject to a hierarchical clustering analysis.\n",
    "    * Similarity of the internal representations corresponded to lexical items with similar properties (words with similar meanings), as evident by being grouped together lower in the tree:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersHistoryElman1990DiscoveringLexicalClasses.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00674a2",
   "metadata": {},
   "source": [
    "### [Generating Text with Recurrent Neural Networks (2011) Sutskever, Martens, Hinton](https://icml.cc/2011/papers/524_icmlpaper.pdf)\n",
    "\n",
    "* The power of large RNNs for predicting the next character in a stream of text (character-level language modeling).\n",
    "\n",
    "* The architecture used was called 'Multiplicative RNN\" and is explained in detail in the paper. Thousands of neurons, millions of connections. \n",
    "\n",
    "* Oddly enough, the motivation stated (for character-level language modeling) was to improve compression of text files. \n",
    "\n",
    "* Trained on over 100 MB of text (Wikipedia, NYT) for several days using 8 GPUs in parallel (!). \n",
    "\n",
    "* After training: \n",
    "    * Generate 'language' by feeding the output as the next input. \n",
    "    * Prompt: \"The meaning of life is\" <br> Response: \"The meaning of life is the tradition of the ancient human repro- duction: it is less favorable to the good boy for when to remove her bigger. In the show’s agreement unanimously resurfaced. The wild pasteured with consistent street forests were incorporated by the 15th century BE. In 1996 the primary rapford undergoes an effort that the reserve conditioning, written into Jewish cities, sleepers to incorporate the .St Eurasia that activates the popula- tion. Mar??a Nationale, Kelli, Zedlat-Dukastoe, Florendon, Ptu’s thought is. To adapt in most parts of North America, the dynamic fairy Dan please believes, the free speech are much related to the\"\n",
    "    \n",
    "* The MRNN learned something about language but [could not hold a 'coherent train of thought'](https://www.technologyreview.com/2016/08/09/158125/ais-language-problem/) through long sequences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1060dae",
   "metadata": {},
   "source": [
    "### [The Unreasonable Effectiveness of Recurrent Neural Networks (2015) Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) \n",
    "\n",
    "* Bigger network, more layers, (same) Character-Level Language Models. \n",
    "\n",
    "* RNNs: baby names, Shakespeare.  \n",
    "\n",
    "* Multi-layer LSTMs: raw Wikipedia (structured markdown), Algebraic Geometry book (raw Latex), Linux Source Code. \n",
    "\n",
    "<br> \n",
    "<br> \n",
    "\n",
    "But still...\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersWillKnight2016.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fbe38",
   "metadata": {},
   "source": [
    "### [Learning to Generate Reviews and Discovering Sentiment (2017)  Radford, Jozefowicz, Sutskever](https://arxiv.org/abs/1704.01444)\n",
    "\n",
    "* Larger network still, trained on (83M) Amazon reviews. \n",
    "\n",
    "* The model chosen for the large scale experiment is a single layer multiplicative LSTM (Krause et al., 2016) with 4096 units and $\\sim$80M parameters.\n",
    "\n",
    "* Character-level modeling: text is processed as a sequence of UTF-8 encoded bytes (Yergeau, 2003).\n",
    "\n",
    "* Training took approximately one month.\n",
    "\n",
    "* Some neurons in the hidden layers were found to encode high-level concepts: \n",
    "    * \"we find a single unit which performs sentiment analysis\" \n",
    "    * The 'sentiment neuron' was not specifically trained for the task. It was learned in an unsupervised manner while training to predict the next word. \n",
    "    \n",
    "* Generating text while **forcing** the sentiment neuron to be positive or negative generated postitive or negative reviews, **respectively**: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersSentimentNeuronReviews.png\" width=\"550\">\n",
    "\n",
    "* There may well be undiscovered interesting neurons there still... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132fc5e7",
   "metadata": {},
   "source": [
    "### The RNN bottleneck \n",
    "\n",
    "All of these models started to fail (lose coherence as a language model) when the sequences became long. \n",
    "\n",
    "All RNNs process the data serially - one character at a time - such that their internal **fixed number** of 'context neurons' eventually reaches capacity and loses track of the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb079d",
   "metadata": {},
   "source": [
    "### Word embedding \n",
    "\n",
    "#### Word Vectors\n",
    "\n",
    "*Word2Vec*\n",
    "\n",
    "Word vectors numerical representation of text. Unlike on-hot-encoding, the encoding of a given word is based on a **Continuous Bag-of-Words model (CBOW)** or a **Continuous Skip-gram model**.\n",
    "\n",
    "For instance, consider: \n",
    "\n",
    "“We propose two novel model architectures for computing continuous vector representations of words from very large data sets. <span><font color=\"purple\"> The quality of these </font></span> <span><font color=\"red\">  representations </font></span>  <span><font color=\"purple\"> is measured in a </font></span> word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.”\n",
    "\n",
    "* <span><font color=\"red\">  representations </font></span> is the focus word. \n",
    "* <span><font color=\"purple\"> The quality of these </font></span> and   <span><font color=\"purple\"> is measured in a </font></span> are context words. \n",
    "* CBOW: \n",
    "    * Linear network with input layer, projection layer (simple matrix multiplication without the non-linear activation function), output layer. \n",
    "    * BUT:  **one** weight matrix between the input and the hidden layer is **shared** for all context words and their projections (products of the word and the weight matrix) are averaged. \n",
    "    * Words are one-hot-encoded.\n",
    "    * Context words are the input to a neural network. \n",
    "    * Training task: maximise the conditional probability of observing the actual output word (the focus word) given the input context words. For instance, given the input {“the”, “quality”, “of”, “these”, “are”, “measured”, “in”, “a”} we want to maximise the probability of getting “representation” as the output.\n",
    "    * The hidden layer activation pattern of the trained network is the  numerical representation of an input word. \n",
    "* Continuous Skip-gram model\" \n",
    "    * The mirror image of the CBOW.\n",
    "    * Input: focus word <br> Target output: predict the context words\n",
    "    \n",
    "\n",
    "* [Efficient Estimation of Word Representations in Vector Space (2013) Mikolov et al.](https://arxiv.org/pdf/1301.3781)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality (2013) Mikolov et al.](https://arxiv.org/pdf/1310.4546)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersWordEmbedding2013.png\" width=\"550\">\n",
    "\n",
    "<br> \n",
    "\n",
    "*BERT*\n",
    "* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2019) Devlin et al.](https://arxiv.org/pdf/1810.04805)\n",
    "\n",
    "* Uses tranbsformers. Explained [here](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f394c7",
   "metadata": {},
   "source": [
    "### [Attention Is All You Need (2017) Vaswani et al.](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "\n",
    "* Task: translating between languages. \n",
    "    * Input: English text\n",
    "    * Output French translation\n",
    "\n",
    "* Attention: a layer with **dynamic** connections that can adapt based on the context of the input ('self-attention layer').\n",
    "    * Every word in the input compare itself to all the other words in, say, the same sentence (or paragraph, or...). \n",
    "    * The embedding of the input word is **adjusted** based on the the embeddings of the most relevant words, to **better capture its context**.  \n",
    "    * So each embedding vector is **TRANSFORMED** based on the words around it. <br> The embedding (encoding of words) itself becomes dynamic. \n",
    "    \n",
    "\n",
    "### [Language Models are Unsupervised Multitask Learners (2018) Radford et al.](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* GPT uses the transformer architechture to generate text (rather than translate) by feeding the output back as the next input. \n",
    "\n",
    "* GPT-1 trained on 7,000 books and captured hundred of context words. <br>  GPT-2 (300K neurons) trained on all popular outbound links from Reddit. \n",
    "\n",
    "* New capabilities: \n",
    "   * Zero-shot behavbior: answering general questions (not from the training data) \n",
    "   * Reading comprehension\n",
    "   * Summarization \n",
    "   * Reasoning \n",
    "   * Language translation (without specifically being trained for it) \n",
    "   \n",
    "But enevtually the output from GPT-2 drifted to nonsense. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911d29d",
   "metadata": {},
   "source": [
    "### [Language Models are Few-Shot Learners (2020) Brown et al.](https://arxiv.org/pdf/2005.14165)\n",
    "\n",
    "* GPT-3 was 100 times bigger, with 175B parameters and a context window of $\\sim$1000 words.  \n",
    "\n",
    "* Trained on 'the internet'. \n",
    "\n",
    "* New capability: \n",
    "    * In-context learning: after training was complete the network could **still learn** new things. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersInContextLearning.png\" width=\"550\">\n",
    "\n",
    "\n",
    "(Similar to the [wug test](https://www.oxfordreference.com/display/10.1093/oi/authority.20110803125127433))\n",
    "\n",
    "* Note: <br> the weights of the **trained** network are 'frozen', yet it can still chagne in response to new information! \n",
    "    * To the best of our knowledge, in-context learning requires encoding internal models of high level (abstract) concepts. \n",
    "    * These models can be combined in any which way - the combinatorics are staggering. New conclusions can be **inferred**.\n",
    "    * In other words: an understanding (of sorts?) of what the language represents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d8459",
   "metadata": {},
   "source": [
    "### [Sparks of Artificial General Intelligence: Early experiments with GPT-4 (2023) Bubeck et al.](https://arxiv.org/pdf/2303.12712)\n",
    "\n",
    "\"Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"\n",
    "\n",
    "A very interesting read. \n",
    "\n",
    "### [GPT-4 Technical Report (2024) OpenAI](https://arxiv.org/pdf/2303.08774)\n",
    "\n",
    "\"GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers.\"\n",
    "\n",
    "<br> \n",
    "\n",
    "* Interestingly, a sufficiently large LLM can be given sensors and actuators to interact with the physical world (through appropriate APIs) and can in-context learn to perform mechanical tasks.  \n",
    "\n",
    "* Generally, instead of specialized networks for image processing, sound processing, natural language processing, mechanical feedback processing, video processing, etc., many different tasks can be viewed as a 'language': a sequence of symbols that carry information and a prediction/generation task (learned using self-attention networks). \n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7325321c",
   "metadata": {},
   "source": [
    "### [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits (2024) Ma et al.](https://arxiv.org/pdf/2402.17764)\n",
    "\n",
    "* In order to run larger and larger models, more and more computation resources are requoired (memory, time, energy). <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersAltmanDavos.png\" width=\"450\">\n",
    "    * Training GPT-3 is estimated to have required $>1300MWh$ - roughly the amount of energy consumed by all households in the Chicago for an hour (or $110$ households for a year). \n",
    "    * Performing tasks by trained network is $\\sim 1B$ times cheaper per text generation task (more expensive for images), but many people prequire many tasks...  \n",
    "    * Globally, data centers are estimated to consume as much energy as a fair-sized country (UK). They are likely to require more. \n",
    "\n",
    "* Reducing the size of LLMs is therfore a conputational and environmental goal. \n",
    "\n",
    "* Post-training quantization: reducing the precision of the weights of the model.\n",
    "    * E.g., from float16 (two bytes) to int8 (one byte).\n",
    "    * This may reduce the performance of the model. \n",
    "\n",
    "* Ma et al. show that weights $\\in \\{-1,0,1\\}$ ($\\log_2(3) \\simeq 1.58 $ bit) can be done without reducing perfromance:  \n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformers1-bitLLMs.png\" width=\"550\">\n",
    "\n",
    "<br> \n",
    "\n",
    "* The 1.58-bit model is trained with weights $\\in \\{-1,0,1\\}$. It is not post-training quantized. \n",
    "\n",
    "* Note: \n",
    "    * The quantized model \"requires almost no multiplication operations for matrix multiplication and can be highly optimized\". \n",
    "    * The \"modeling capability is stronger \\[as compared to using  weights $\\in \\{-1, 1\\}$ \\] due to its explicit support for feature filtering, made possible by the inclusion of 0 in the model weights\" \n",
    "    \n",
    "* How does the model constrain weights to $\\{-1,0,1\\}$? <br> Absolute mean quantization: scale the weight matrix by its average absolute value, and then round each value to the nearest integer $\\in \\{-1, 0, +1\\}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec6c6c6",
   "metadata": {},
   "source": [
    "### [The Curious Decline of Linguistic Diversity (2024) Guo et al.](https://arxiv.org/pdf/2311.09807)\n",
    "\n",
    "### [Will Large-scale Generative Models Corrupt Future Datasets (2023) Hataya et al.](https://arxiv.org/pdf/2211.08095)\n",
    "\n",
    "* Where does the training data come from? \n",
    "    * Until 2023, practically all of it was created by humans.\n",
    "\n",
    "* The more AI is used to produce content, the higher the chances that future models will train on significant portions of data that was generated by previous models. \n",
    "    * It is unlcear whether this is a problem and, if so, how big of a problem. \n",
    "    \n",
    "* Two potential problems:\n",
    "    * Convergent thinking AI training on AI data training on AI data (...) will produce less variable outputs. \n",
    "    * Divergent thinking: AI training on AI data training on AI data (...) will randomly produce results further away from human experiences and real world constraints.  \n",
    "\n",
    "* Guo et al. tested measures for the diversity of language on an open source LLM called [OPT](https://arxiv.org/abs/2205.01068) performing tasks requiring different levels of creativity. \n",
    "    * Summarizing an article\n",
    "    * Generating a scientific abstract\n",
    "    * Generating a story\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersDeclineLinguisticDiversity.png\" width=\"550\">\n",
    "\n",
    "* Rows in the table indicate iterations of recursive training simulation: training each model on the text generated by its predecessor model (starting from text generated by humans).  \n",
    "\n",
    "* The table shows a consistent decline in the linguistic diversity of the model outputs through successive iterations. \n",
    "\n",
    "* The decline is most prominent for tasks demanding high levels of creativity. \n",
    "\n",
    "* \"This trend underscores the potential risks of training language models on synthetic text, particularly concerning the preservation of linguistic richness.\"\n",
    "\n",
    "* A similar point was made for iterative training of image generating (stable diffusion) models. \n",
    "\n",
    "Original images: \n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersDeclineImagesOriginal.png\" width=\"550\">\n",
    "\n",
    "\n",
    "Images generated after a few iterations: \n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersDeclineImagesIterations.png\" width=\"550\">\n",
    "\n",
    "<br> \n",
    "\n",
    "But also note the decline in overall diversity: \n",
    "| | | \n",
    "|:-:|:-:| \n",
    "| Original | After iterations |\n",
    "| <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersDeclineImagesOriginal2.png\" width=\"275\"> | <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersDeclineImagesIterations2.png\" width=\"275\"> | \n",
    "\n",
    "<br> \n",
    "<br> \n",
    "\n",
    "### Open questions \n",
    "\n",
    "#### Is AI generated data a form of pollution? \n",
    "\n",
    "#### Can regulation require marking of AI generated content (and is it necessary)? \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Does AI in the long term *require* human creativity? \n",
    "\n",
    "#### Can the next generation of AI build in diversity and creativity (at a human level)? \n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304b1da",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_architecture.png\" width=\"600\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d76d8f",
   "metadata": {},
   "source": [
    "### The Encoder \n",
    "\n",
    "Words $\\rightarrow$ context vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb333ff",
   "metadata": {},
   "source": [
    "### 1. Word embedding\n",
    "\n",
    "A network that encodes words in vectors of a given length. \n",
    "\n",
    "Example: one-hot-encoding $\\rightarrow$ BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fd7aa",
   "metadata": {},
   "source": [
    "### 2. Positional encoding\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_positional_encoding_gate.png\" width=\"150\"> \n",
    "\n",
    "The order (positions) of words in a sentence matters - changing the order can change the meaning. \n",
    "\n",
    "The transformer model treats each data point as independent (to enable parallel processing $\\rightarrow$ scalability). Therefore, positional information is added explicitly to keep track of the order of words in a sentence. \n",
    "\n",
    "\n",
    "* For each word in the vocabulary, calculate a positional vector that has the same dimension as the word embedding vector, $d$.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_positional_encoding_1.png\" width=\"500\"> \n",
    "\n",
    "* Positional vectors are derived from trigonometric functions: \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_positional_encoding_2.png\" width=\"500\"> \n",
    "\n",
    "$\\begin{eqnarray}\n",
    "L &=& \\text{length of sentence} \\ \\ \\ , \\ \\ \\ \n",
    "k = 0 \\dots \\left\\lceil \\frac L2 \\right\\rceil \\\\\n",
    "&& \\\\ \n",
    "d &=& \\text{dimension of the word embedding vector} \\ \\ \\ , \\ \\ \\ \n",
    "i = 0 \\dots \\left\\lceil \\frac d2 \\right\\rceil \\\\\n",
    "&& \\\\\n",
    "p(\\ell, 2i) &=& \\sin \\left( \\frac{k}{10,000^{2i/d}}  \\right) \\ \\ \\ , \\ \\ \\ \n",
    "p(\\ell, 2i+1) = \\cos \\left( \\frac{k}{10,000^{2i/d}}  \\right) \\\\\n",
    "&& \\\\ \n",
    "\\omega_0 &=& \\frac 1{2\\pi \\ \\underbrace{10,000}_n} \\text{ is a hyperparameter (value used in 'Attention is all you need')} \n",
    "\\end{eqnarray}$ \n",
    "\n",
    "* For instance, if the length of the sentence is $L=4$, the dimension of the embedding vector is $d=4$, and the hyperparameter is $n=100$ then:  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_positional_encoding_3.png\" width=\"500\"> \n",
    "\n",
    "* Finally, add the positional encoding vectors to the word embedding vectors to get the 'word embedding $+$ positional encoding' vectors for each word in the sentence:  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_positional_encoding_4.png\" width=\"500\"> \n",
    "\n",
    "* This allows the transformer model to **keep track of the order of words** (as well as their identity and context). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abccd6f",
   "metadata": {},
   "source": [
    "### 3. Attention (or self attention)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_attention.png\" width=\"120\"> \n",
    "\n",
    "\n",
    "\n",
    "Natural language is often ambiguous. \n",
    "\n",
    "For instance: \n",
    "\n",
    "Get me one <span> <font color=\"purple\"> apple </font></span>  and one banana.<br> \n",
    "vs.<br>\n",
    "<span> <font color=\"purple\"> Apple </font></span>  announced its new microchips. \n",
    "\n",
    "\n",
    "* Disambiguating a sentence requires:\n",
    "    * Associating tokens with other tokens correctly. \n",
    "    * More focus on important pieces of information (tokens that provide context).\n",
    "\n",
    "<br> \n",
    "\n",
    "#### The attention network TRANSFORMS the embedding of the key word based on words in its vicinity\n",
    "\n",
    "| | | \n",
    "|:-:|:-:|\n",
    "| <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersAttentionChangeEmbeddingNumbers.png\" width=\"350\"> | <img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersAttentionChangeEmbeddingPlot.png\" width=\"200\"> |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aac281",
   "metadata": {},
   "source": [
    "#### ChatGPT can do this:  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_ChatGPT_1.png\" width=\"600\"> \n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/Transformer_ChatGPT_2.png\" width=\"600\"> \n",
    "\n",
    "<br> \n",
    "\n",
    "**How?** \n",
    "\n",
    "* Calculate **similarities** between each pair of words in the sentence, where the ranking is determined by how closely the given pair of words appear in a large body of text (corpus). \n",
    "    * E.g., if 'it' is more commonly associated with 'horse' than with 'stable' then the similarity scores will make the encoding of 'horse' impact the encoding of 'it' more. \n",
    "\n",
    "\n",
    "* In practice, to determine the attention of a given token the transformer calculates\n",
    "    * A query (for the given word)\n",
    "    * Keys (for all words in the vicinity, including the given tokens)\n",
    "    * A values (for all words in the vicinity, scaled by similarities and used to calculate the new representation of the given word) \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69074c24",
   "metadata": {},
   "source": [
    "* **Query** for the word under consideration: \n",
    "    * A vector of dimension $d$ that is calculated from the 'word embedding $+$ positional encoding' vector using a simple network (a projection layer, which is a matrix).  \n",
    "    * A single network creates all queries - same set of weights for all words. \n",
    "\n",
    "* **Keys** for all words in the same sentence/paragraph/...: \n",
    "    * Vectors of dimension $d$ that are calculated from the 'word embedding $+$ positional encoding' vectors using a simple network (a projection layer, which is a matrix). \n",
    "    * A single network creates all keys - same set of weights for all words. \n",
    "    \n",
    "* **Similarities** between the quary and each of the keys:     \n",
    "    * The (cosine) similarity between a query and a key is calculated using the dot product: <br/> $similarity = \\vec{query} \\cdot \\vec{key}$ <br> This is done for every query-key pair of tokens (including every pair of words in the sentence). \n",
    "    * Importantly, the similarity of each word to **itself** is also calculated. It is always high and therefore a word's origninal encoding will be an important part of its encoding that includes attention information.  \n",
    "    * The required weights are trained on a large dataset to produce larger similarity values between 'it' and 'horse' as compared to 'it' and 'stable'. <br> Likewise, a larger similarity should be calculated for 'where' and 'stable' as compared to 'where' and 'wet'. \n",
    "    * The similarity scores are passed through a softmax function (numbers $\\rightarrow$ probabilities). These are used as normalized fractions for combining vales (see below) to get an encoding for the key that **'pays attention'** to pertinant information.\n",
    "    \n",
    "* **Values** to be scaled and combined to create the new word-vector:  \n",
    "    * The 'embedding+positional' encodings are not simply combined in a weighted average.      \n",
    "    * A simple network (a projection layer, which is a matrix) calculates a 'value' vector of dimension $d$ from each 'embedding+positional' encoding.\n",
    "    * A single network creates all values - same set of weights for all words. \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ed0a1",
   "metadata": {},
   "source": [
    "#### Why bother with the Keys, Queries, and values matrices? \n",
    "\n",
    "* A key and a query matrix define a linear transformation (that is learned). \n",
    "\n",
    "* The linear transformation allows to optimize the embedding **for the purpose of calculating similarities**: \n",
    "    * Matrix elements (weights) are learned for best separation between different contexts, i.e., for **disambiguation**.  \n",
    "    * This depends largely on lexical catregories of single words (or similarly abstract features).  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersKeysQueriesLinearTransformation.png\" width=\"400\"> \n",
    "\n",
    "* The best final embedding for all the words is optimized on the task of finding the next word. \n",
    "    * It is generally different from the best embedding for disambiguation based on context. \n",
    "    * Hence, the values matrix. \n",
    "    * This depends on proximity of words in sentences (because of grammar and other reasons). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b447dfc",
   "metadata": {},
   "source": [
    "#### Attention (self-attention):     \n",
    "\n",
    "* The **weighted average of the value vectors** is calculated using the normalized **similarities of the query and keys** to produce the 'context vectors'.  \n",
    "    * The similarities are normalized with softmax: $$s_i \\to \\frac{e^{s_i}} {\\sum_k e^{s_k}}$$\n",
    "    * Value vectors for all words in the sentence (or para or...) are scaled by the corresponding normalized similarity and added: \n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersSimilaritiesAndSoftmax.png\" width=\"500\"> \n",
    "\n",
    "* Now 'Orange' has some 'Apple' in it, and less of the less relevant words. \n",
    "\n",
    "* When all of the vectors are updated, the entire embedding is transformed to better replect the context.  \n",
    "\n",
    "<br> \n",
    "\n",
    "* This attention encoding contains **contextual information** from all other words in the sentence. This can help determine how a subject pronoun, for instance, related to each of two potential subjects.  \n",
    "\n",
    "* **Parallel processing**: the queries, keys, and values vectors for all words can be calculated in parallel. Once the respective networks are trained, there are no dependencies - each vector only needs the 'embedding+positional' encoding of its own word. <br> Likewise, similarities between different pairs of words can be calculated in parallel. <br> GPUs and/or cloud computing enable to process many words relatively quickly. \n",
    "\n",
    "* **Multi-head attention**: \n",
    "    * A stack of attention cells (each with its own set of wights) can determine attention in clauses, sentences, paragraphs, etc. This would capture, e.g., local interactions and long-distance dependencies. \n",
    "    * Redundancy (e.g., two cells per sentence) can to generalization and robustness. \n",
    "    * The independent attention cells can be computed in parallel. \n",
    "    * The independent attention outputs (context vectors) are then concatenated and linearly transformed into the desired dimension.\n",
    "    *  In 'All you need is attention' the authors stacked $8$ attention cells. \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92560a65",
   "metadata": {},
   "source": [
    "#### Interim Summary: Simplified Transformer Diagram\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/david-biron/DATA221imgs/main/TransformersSimplifiedTransformerDiagram.png\" width=\"450\">\n",
    "\n",
    "<br> \n",
    "\n",
    "And this is [a beautiful visualization of attention in transformers](https://www.youtube.com/watch?v=eMlx5fFNoYc) (differing in some details)\n",
    "\n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf19718",
   "metadata": {},
   "source": [
    "### 4. Residual connections (skip connections):  \n",
    "\n",
    "* The 'embedding+positional' encoding is added to the 'multi-attention' encoding without intermediate weights. \n",
    "* Such residual connections help mitigate vanishing gradient issues. \n",
    "* More generally, skip connections mitigate the degradation in performance as the network depth increases.  <br/> Deeper neural networks deteriorate in performance compared to shallower counterparts: it becomes more challenging for the model to learn effective representations of the data. Examples include\n",
    "    * Vanishing or exploding gradients issues.\n",
    "    * Overfitting.\n",
    "    * 'Information bottleneck': as the data passes through multiple layers, each layer extracts increasingly abstract / high-level features; if the network is too deep, it may not have capacity to represent all the necessary information, resulting in a loss of important details. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b797fa",
   "metadata": {},
   "source": [
    "## The Decoder (in brief) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc006c2",
   "metadata": {},
   "source": [
    "### 1. Decoder word embedding for the output vocabulary\n",
    "\n",
    "(train on relevant corpus, e.g., a different language if the model is used for machine translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afbaf0c",
   "metadata": {},
   "source": [
    "### 2. Decoder positional encoding for output vocabulary\n",
    "\n",
    "(same networks/weights used for the input vocabulary) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bdaf07",
   "metadata": {},
   "source": [
    "### 3. Decoder attention layer \n",
    "\n",
    "(train on relevant corpus - use **different** Query, Key, and Value weights/network from those in the Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb990a",
   "metadata": {},
   "source": [
    "### 4. Encoder-Decoder attention\n",
    "\n",
    "* **Query**: calculate a query for each token-context-vector in the **output (decoder) vocabulary**. \n",
    "\n",
    "* **Key**: calculate a key for each token-context-vector in the **intput (encoder) vocabulary**. \n",
    "\n",
    "* **Similarities**: calculate cosine similarities (dot products) between the **decoder queries** and the **encoder keys**. \n",
    "\n",
    "* **Softmax**: transform the similarities to probabilities (normalized fractions). \n",
    "\n",
    "* **Values**: calculate values for each token-context-vector in the **intput (encoder) vocabulary**.\n",
    "\n",
    "* **Encoder-Decoder attention vectors**: scale the input (context) values by the softmax probabilities and add to get the weighted average.  \n",
    "    * Recall: the quaries came ffrom the decoder. \n",
    "    * Note: the weights used for calculating queries, keys, and values for the Encoder-Decoder attention are **different** from those used for self attention. \n",
    "    * Like before: the weights used for calculating queries, keys, and values for the Encoder-Decoder attention are the same for each word, so that this calculation can be performed in parallel. \n",
    "    \n",
    "    \n",
    "* **Multi-head attention**: Encoder-Decoder attention cells can be stacked to capture interactions at different scales (e.g., clause, sentence, paragraph) and increase robustness.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf6b4c",
   "metadata": {},
   "source": [
    "### 5. Residual connections (skip connections) \n",
    "\n",
    "* Add the Encoder-Decoder attention vectors to the **Decoder** 'embedding+positioning' vectors (with no intermediate weights) to mitigate degradation in performance. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457c293",
   "metadata": {},
   "source": [
    "### 6. Full connected (linear) layer\n",
    "\n",
    "Pass the resulting vector through a linear layer to select a token from the output vocabulary:\n",
    "\n",
    "* Input dimension: dimension of context vector.\n",
    "\n",
    "* Output dimension: dimention of Decoder vocabulary. \n",
    "\n",
    "* Select a token using softmax. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c7b18a",
   "metadata": {},
   "source": [
    "### 7. Stopping condition\n",
    "\n",
    "* The Decoder will stop when it outputs an 'end of something' token. \n",
    "\n",
    "* Each output will be:\n",
    "    * Word embeded \n",
    "    * Position encoded\n",
    "    * Self-attentioned (+ residual connections) \n",
    "\n",
    "* The result will be fed into the Encoder-Decoder attention calculation:\n",
    "    * Query from decoder (result from above) \n",
    "    * Keys and Values from the Encoder. \n",
    "\n",
    "* The result will be added to the decoder context vector using skip connections. \n",
    "\n",
    "* The result will be passed through the same linear layer (+softmax) used for chosing an output token in the previous stage. \n",
    "    * If the output is the end token - decoding is done. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d796939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
