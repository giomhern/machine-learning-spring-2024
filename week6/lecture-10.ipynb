{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ce827a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "\n",
    "### DATA 22100 - Introduction to Machine Learning\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://github.com/david-biron/DATA221imgs/blob/main/UChicago_DSI.png?raw=true\" align=\"right\" alt=\"UC-DSI\" width=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb79567",
   "metadata": {},
   "source": [
    "<center> \n",
    "\n",
    "# Neural Networks in a nutshell\n",
    "    \n",
    "<br/>\n",
    "    \n",
    "</center> \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff2f91",
   "metadata": {},
   "source": [
    "### Neurons \n",
    "\n",
    "A **neuron** is an electrically excitable cell that fires electric signals called action potentials. \n",
    "\n",
    "<img src=\"https://github.com/david-biron/DATA221imgs/blob/main/neuron.png?raw=true\" width=\"500\">\n",
    "\n",
    "\n",
    "In a *grossly oversimplified* picture, a neuron would: \n",
    "\n",
    "* receive all or none ($1$ or $0$) input signals via its **dendrites** (and soma). \n",
    "* multiply each input by a 'weight' (**synaptic strength**). \n",
    "* aggregate the weighted input signals in the **soma** (and dendrites), <br/> i.e., sum the weighted signals. \n",
    "* pass the sum through a non-linear function that determines whether to send a $1$ or $0$ output signal. \n",
    "* send out an output signal (or not) to other neurons down the **axon**, <br/> which would connect to dendrites of downstream neurons. \n",
    "\n",
    "\n",
    "The signaling process is electro-chemical: \n",
    "\n",
    "* Input electric action potentials trigger chemical signaling in the receiving dendrite. \n",
    "* Chemical signaling in the dendrites and the soma aggregate the inputs and, eventually, trigger (or not) the output electric action potential.  \n",
    "* The structure connecting an upstream axon to a downstream dendrite is called a **synapse**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e13c13",
   "metadata": {},
   "source": [
    "### Simple artificial 'neurons'\n",
    "\n",
    "Computational neural networks are composed of simple units ('neurons') loosely inspired by biological neurons: \n",
    "\n",
    "| | | \n",
    "|:-:|:-:|\n",
    "| <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/neuronArtificial1.png?raw=true\" width=\"400\"> | <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/neuronArtificial2.png?raw=true\" width=\"500\"> | \n",
    "\n",
    "* Inputs ($1$ or $0$) from other neurons are multiplied by 'synaptic' weights $w_j$\n",
    "* Weighted inputs are summed ($\\sum$), optionally along with a 'bias' ($b$). \n",
    "* The sum is passed through a nonlinearity ($\\cal S$) that transforms it to a $0$ or $something$ output. <br/> The non-linearity is also called the **activation function**, $g()$.  \n",
    "* The output can serve as input to downstream neurons. \n",
    "\n",
    "<center>    \n",
    "$\\begin{eqnarray}\n",
    "\\boxed{ \\  \\text{single_neuron_output} = \\text{activation_function} \\left( \\sum_j \\text{weight}_j \\ \\times \\ \\text{input}_j + \\text{bias} \\right)  \\ } \n",
    "\\end{eqnarray}$ \n",
    "</center>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36340e42",
   "metadata": {},
   "source": [
    "#### A simple neuron example \n",
    "\n",
    "|   |   |\n",
    "|:--|:--|\n",
    "| <img src=\"https://github.com/david-biron/DATA221imgs/blob/main/icon_example.png?raw=true\" width=\"50\"> | Consider a 'neuron' with two inputs ($24$ and $16$), where the corresponding weights are $0.5$ and $-1$, <br/> there is no bias, and the activation function is a Rectifier Linear Unit, where $ReLU(x) = \\max\\left(0, x\\right)$.  |\n",
    "\n",
    "<img src=\"https://github.com/david-biron/DATA221imgs/blob/main/ExampleNeuronWithTwoInputs.png?raw=true\" width=\"500\">\n",
    "\n",
    "Input 1: $24 \\times 0.5 = 12$ <br/>\n",
    "Input 2: $16 \\times (-1.0) = -16$ <br/>\n",
    "Sum ($\\sum$): $12 + (-16) = -4$ <br/>\n",
    "Output $ = ReLU(-4) = 0$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Input 1: $36 \\times 0.5 = 18$ <br/>\n",
    "Input 2: $16 \\times (-1.0) = (-16)$ <br/>\n",
    "Sum ($\\sum$): $12 + (-16) = 2$ <br/>\n",
    "Output $ = ReLU(2) = 2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca6572",
   "metadata": {},
   "source": [
    "### Commonly used activation functions include:\n",
    "\n",
    "* [`Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)\n",
    "* [`Tanh`](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html) (hyperbolic tangent). \n",
    "* [`ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) (Rectified Linear Unit). \n",
    "\n",
    "<img src=\"https://github.com/david-biron/DATA221imgs/blob/main/CommonActivationFunctions2.png?raw=true\" width=\"500\"> \n",
    "\n",
    "<br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e20a1",
   "metadata": {},
   "source": [
    "### Artificial Neural Networks\n",
    "\n",
    "* An **Artificial Neural Network** is composed of multiple (many) artifical neurons that are connected to each other, <br> i.e., the output of some serve as inputs of others.\n",
    "* A common architecture arranges the neurons in **layers**: an input layer, multiple hidden layers, and an output layer (left to right). \n",
    "\n",
    "<img src=\"https://github.com/david-biron/DATA221imgs/blob/main/ArtificialNN1.png?raw=true\" width=\"500\">\n",
    "\n",
    "* Fully connected layers (every node in the upstream layer is connected to every node in the downstream layer) are called **linear layers**.   \n",
    "* **Deep networks** have many (more than one) hidden layers. \n",
    "* A **forward pass** is a signal flowing from left to right: <br> input $\\to$ hidden$_1$ $\\to$ hidden$_2$ $\\to \\ \\dots $ $\\to$ hidden$_n$ $\\to$ output. <br> A prediction is computed with a forward pass.  \n",
    "\n",
    "<br> \n",
    "\n",
    "<center>\n",
    "${ \\begin{eqnarray} \n",
    "\\boxed{ \\\n",
    "\\text{Training a neural network means learning the weights that would produce a desired result.}\n",
    " \\ }\n",
    "\\end{eqnarray}}$ \n",
    "</center>    \n",
    "\n",
    "<br> \n",
    "\n",
    "* A typical training scheme: \n",
    "    * Randomly (or not) **initialize** the weights for all the nodes.\n",
    "    * Given a training example, perform a **forward pass** using the current weights and calculate the output value.\n",
    "    * Compare the output with the target output in the training data: measure the error using a **loss function**.\n",
    "    * Perform a **backwards pass** (from right to left) and use **backpropagation** $+$ **gradient descent** to update the weights. \n",
    "    * Repeat the  **forward pass** and **backwards pass** for each training example, <br> thereby gradually improving the weights.  \n",
    "    \n",
    "* **Backpropagation** (+**Gradient descent**) is a method for calculating each weightâ€™s contribution to the error and adjusting the weights in a direction that **reduces the loss**. \n",
    "    \n",
    "\n",
    "See some examples [here](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6), [here](https://iamtrask.github.io/2015/07/12/basic-python-network/), and [here](https://iamtrask.github.io/2015/07/27/python-network-part2/). \n",
    "\n",
    "<br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073f770",
   "metadata": {},
   "source": [
    "### What is the gradient with respect to the parameter $w_i$? \n",
    "\n",
    "#### Given data $\\{x_1, x_2, \\dots x_p\\}$ \n",
    "\n",
    "Suppose that $\\{w_1, w_2, \\dots w_p\\}$ are a particular layer of weights, e.g., they connect the $2^{nd}$ and $3^{rd}$ layers of neurons.  \n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "\\hat y_{\\text{'w layer'}} &=& w_0 + w_1 x_1 + \\dots w_p x_p \\\\ \n",
    "\\hat p &=& g\\left( \\hat y \\right) \\hspace{1cm} \\text{(activation function)} \\\\ \n",
    "\\text{loss_function} &=& {\\cal L}\\left( \\left. \\hat p \\ , \\ \\text{ground_truth} \\ \\right|  \\ \\text{data, including $x_i$} \\right) \\\\ \n",
    "\\ \\\\ \n",
    "\\rightarrow \\ \\frac{\\partial \\ \\text{loss_function}}{\\partial w_i} &=& \\frac{\\partial {\\cal L}}{\\partial \\hat p} \\frac{\\partial \\hat p}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial w_i} \\\\\n",
    "&\\underbrace{=}_{\\text{e.g. $\\sum (\\dots)^2$ loss}}& 2\\left( \\left. \\hat p \\ - \\ \\text{ground_truth} \\ \\right|  \\ \\text{data} \\right) \\frac{\\partial g}{\\partial \\hat y} x_i\n",
    "\\end{eqnarray}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa867f8c",
   "metadata": {},
   "source": [
    "In a fully connected network (linear layers), the weights are stored in matrices with dimensions corresponding to their upstream/downstream layers. E.g.,:  \n",
    "\n",
    "<img src=\"https://github.com/david-biron/DATA221imgs/blob/main/ArtificialNN2.png?raw=true\" width=\"500\">\n",
    "\n",
    "* The input layer has $3$ nodes. <br/> To fully connect them to the following $4$ nodes we'd need a $3 \\times 4$ matrix of weights.\n",
    "* The first hidden layer has $4$ nodes. <br> To fully connect them to the following $4$ nodes we'd need a $4 \\times 4$ matrix of weights.\n",
    "* The second hidden layer has $4$ nodes. <br> To fully connect them to the output layer we'd need a $4 \\times 1$ matrix of weights (which in the image is depicted as $1 \\times 4$).\n",
    "\n",
    "\n",
    "To make a prediction given an input, the network needs to learn these weights (and the biases and possibly parameters in the activation functions). <br> **Backpropagation** sequentially updates these weights until they are 'learned'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f3a3a",
   "metadata": {},
   "source": [
    "### A Neural Network Playground\n",
    "\n",
    "To get a feel for NNs, [try this out](https://playground.tensorflow.org/#activation=relu&batchSize=26&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=3,2,2&seed=0.58949&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=false&ySquared=false&cosX=false&sinX=true&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&batchSize_hide=false). \n",
    "\n",
    "<br> \n",
    "\n",
    "(don't forget to train the network with the 'Play' button on the top left). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7ba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
